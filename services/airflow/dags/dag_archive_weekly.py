"""
DAG Archive Weekly - RÃ©cupÃ©ration hebdomadaire des donnÃ©es historiques
ExÃ©cution : Tous les dimanches Ã  01:00 (avant le monitoring Ã  02:00)

TÃ¢ches :
- Ingestion mÃ©tÃ©o archive (semaine passÃ©e) â†’ stg_weather_archive
- Ingestion mÃ©tÃ©o forecast (J+7) â†’ stg_weather_forecast  
- Ingestion transport archive (semaine passÃ©e) â†’ stg_transport_archive

Note: 
- L'entraÃ®nement du modÃ¨le est gÃ©rÃ© par le DAG monitoring_weekly
  qui le dÃ©clenche uniquement si un drift est dÃ©tectÃ©.
- Les dates sont calculÃ©es automatiquement via Airflow (logical_date).
- Pour un POC, la frÃ©quence hebdomadaire est suffisante.
"""

from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.operators.empty import EmptyOperator
from datetime import datetime, timedelta
import sys
import os
import logging

# Ajouter le chemin des scripts pipeline
sys.path.insert(0, "/opt/project/pipeline")

# Configuration du logging
logger = logging.getLogger(__name__)

# Configuration par dÃ©faut du DAG
default_args = {
    "owner": "delay-forecast",
    "depends_on_past": False,
    "email_on_failure": False,
    "email_on_retry": False,
    "retries": 1,
    "retry_delay": timedelta(minutes=5),
}

# Configuration pour les appels API (retry avec backoff)
api_task_args = {
    "retries": 4,
    "retry_delay": timedelta(minutes=1),
    "retry_exponential_backoff": True,
    "max_retry_delay": timedelta(minutes=15),
    "execution_timeout": timedelta(minutes=10),
}

# Configuration pour les tÃ¢ches ETL
etl_task_args = {
    "retries": 2,
    "retry_delay": timedelta(minutes=2),
    "execution_timeout": timedelta(minutes=30),
}

# CoordonnÃ©es Stockholm
LAT, LON = 59.3251172, 18.0710935


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS UTILITAIRES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def get_week_dates_from_context(context):
    """
    Calcule les dates de la semaine Ã  partir du contexte Airflow.
    
    Args:
        context: Contexte Airflow
    
    Returns:
        tuple: (start_date, end_date) au format string YYYY-MM-DD
        
    Notes:
        - data_interval_start : dÃ©but de la pÃ©riode de donnÃ©es
        - data_interval_end : fin de la pÃ©riode de donnÃ©es
        - Pour un DAG hebdomadaire, cela correspond Ã  la semaine prÃ©cÃ©dente
    """
    # Utiliser data_interval pour Ãªtre prÃ©cis
    data_interval_start = context.get('data_interval_start')
    data_interval_end = context.get('data_interval_end')
    
    if data_interval_start and data_interval_end:
        start_date = data_interval_start.strftime('%Y-%m-%d')
        end_date = (data_interval_end - timedelta(days=1)).strftime('%Y-%m-%d')  # -1 car end est exclusif
    else:
        # Fallback sur ds (date logique)
        logical_date = context.get('logical_date') or context.get('execution_date')
        end_date = logical_date.strftime('%Y-%m-%d')
        start_date = (logical_date - timedelta(days=6)).strftime('%Y-%m-%d')
    
    return start_date, end_date


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# FONCTIONS DES TÃ‚CHES
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

def task_ingestion_meteo_archive(**context):
    """
    RÃ©cupÃ¨re les donnÃ©es mÃ©tÃ©o historiques pour la semaine.
    Utilise les dates calculÃ©es automatiquement par Airflow.
    """
    from call_api_meteo import fetch_weather_data
    # TODO: ImplÃ©menter upload S3 quand disponible
    # from upload_s3 import send_to_S3
    
    # Dates calculÃ©es depuis le contexte Airflow
    start_date, end_date = get_week_dates_from_context(context)
    execution_date = context.get('ds', 'unknown')
    
    logger.info(f"[{execution_date}] Ingestion mÃ©tÃ©o archive : {start_date} â†’ {end_date}")
    
    try:
        # Appel API avec dates dynamiques
        data = fetch_weather_data(
            LAT, LON,
            filename=f"weather_archive_{start_date}_{end_date}.json",
            mode="archive",
            start_date=start_date,
            end_date=end_date
        )
        
        # TODO: Sauvegarde sur S3 (backup) - dÃ©commenter quand disponible
        # try:
        #     send_to_S3(data, f"weather_archive_{start_date}_{end_date}")
        #     logger.info("âœ… DonnÃ©es sauvegardÃ©es sur S3")
        # except Exception as e:
        #     logger.warning(f"âš ï¸ Ã‰chec sauvegarde S3 (non bloquant) : {e}")
        
        # Passer les donnÃ©es via XCom
        context['ti'].xcom_push(key='meteo_archive_data', value=data)
        logger.info(f"âœ… MÃ©tÃ©o archive rÃ©cupÃ©rÃ©e ({start_date} â†’ {end_date})")
        return data
    except Exception as e:
        logger.error(f"âŒ Ã‰chec ingestion mÃ©tÃ©o archive : {e}")
        raise


def task_ingestion_meteo_forecast(**context):
    """
    RÃ©cupÃ¨re les prÃ©visions mÃ©tÃ©o (J+7).
    Les dates sont calculÃ©es automatiquement (aujourd'hui â†’ J+7).
    """
    from call_api_meteo import fetch_weather_data
    
    execution_date = context.get('ds', 'unknown')
    logger.info(f"[{execution_date}] Ingestion mÃ©tÃ©o forecast")
    
    try:
        # Forecast : dates automatiques (J â†’ J+7)
        data = fetch_weather_data(
            LAT, LON,
            filename=f"weather_forecast_{execution_date}.json",
            mode="forecast"
        )
        
        # TODO: Sauvegarde sur S3 (backup) - dÃ©commenter quand disponible
        # try:
        #     send_to_S3(data, f"weather_forecast_{execution_date}")
        # except Exception as e:
        #     logger.warning(f"âš ï¸ Ã‰chec sauvegarde S3 : {e}")
        
        context['ti'].xcom_push(key='meteo_forecast_data', value=data)
        logger.info("âœ… MÃ©tÃ©o forecast rÃ©cupÃ©rÃ©e")
        return data
    except Exception as e:
        logger.error(f"âŒ Ã‰chec ingestion mÃ©tÃ©o forecast : {e}")
        raise


def task_ingestion_transport_archive(**context):
    """
    RÃ©cupÃ¨re les donnÃ©es transport historiques pour la semaine.
    TÃ©lÃ©charge jour par jour pour la pÃ©riode.
    """
    from call_api_transport import fetch_transport_koda
    # TODO: ImplÃ©menter upload S3 quand disponible
    # from upload_s3 import send_to_S3
    
    # Dates calculÃ©es depuis le contexte Airflow
    start_date, end_date = get_week_dates_from_context(context)
    execution_date = context.get('ds', 'unknown')
    
    logger.info(f"[{execution_date}] Ingestion transport archive : {start_date} â†’ {end_date}")
    
    try:
        # TÃ©lÃ©charger chaque jour de la pÃ©riode
        start_dt = datetime.strptime(start_date, '%Y-%m-%d')
        end_dt = datetime.strptime(end_date, '%Y-%m-%d')
        
        files = []
        current = start_dt
        while current <= end_dt:
            date_str = current.strftime('%Y-%m-%d')
            try:
                filename = fetch_transport_koda(date_str)
                files.append(filename)
                logger.info(f"  ğŸ“¥ {date_str} OK â†’ {filename}")
                
                # TODO: Sauvegarde sur S3 - dÃ©commenter quand disponible
                # try:
                #     send_to_S3(filename, f"transport_archive_{date_str}")
                # except Exception as e:
                #     logger.warning(f"  âš ï¸ S3 {date_str} : {e}")
                    
            except Exception as e:
                logger.warning(f"  âŒ {date_str} Ã©chouÃ© : {e}")
            
            current += timedelta(days=1)
        
        context['ti'].xcom_push(key='transport_archive_files', value=files)
        logger.info(f"âœ… Transport archive : {len(files)} jours rÃ©cupÃ©rÃ©s")
        return files
    except Exception as e:
        logger.error(f"âŒ Ã‰chec ingestion transport archive : {e}")
        raise


def task_etl_meteo(**context):
    """Transforme et charge les donnÃ©es mÃ©tÃ©o vers Neon"""
    from transform_meteo_archives import process_etl_meteo
    from transform_meteo_previsions import process_etl_previsions
    from load_to_neon import load_parquet_to_neon
    
    execution_date = context.get('ds', 'unknown')
    ti = context['ti']
    
    logger.info(f"[{execution_date}] ETL mÃ©tÃ©o")
    
    # RÃ©cupÃ©rer les donnÃ©es depuis XCom
    archive_data = ti.xcom_pull(key='meteo_archive_data')
    forecast_data = ti.xcom_pull(key='meteo_forecast_data')
    
    errors = []
    
    # ETL Archive
    if archive_data:
        try:
            transformed = process_etl_meteo(archive_data)
            load_parquet_to_neon("stg_weather_archive", transformed)
            logger.info("âœ… MÃ©tÃ©o archive â†’ Neon OK")
        except Exception as e:
            logger.error(f"âŒ ETL mÃ©tÃ©o archive : {e}")
            errors.append(str(e))
    
    # ETL Forecast
    if forecast_data:
        try:
            transformed = process_etl_previsions(forecast_data)
            load_parquet_to_neon("stg_weather_forecast", transformed)
            logger.info("âœ… MÃ©tÃ©o forecast â†’ Neon OK")
        except Exception as e:
            logger.error(f"âŒ ETL mÃ©tÃ©o forecast : {e}")
            errors.append(str(e))
    
    if len(errors) == 2:
        raise Exception(f"ETL mÃ©tÃ©o Ã©chouÃ© : {errors}")


def task_etl_transport(**context):
    """Transforme et charge les donnÃ©es transport vers Neon"""
    from transform_transport import process_etl_transport
    from load_to_neon import load_parquet_to_neon
    
    execution_date = context.get('ds', 'unknown')
    logger.info(f"[{execution_date}] ETL transport archive")
    
    try:
        # Les donnÃ©es ont Ã©tÃ© rÃ©cupÃ©rÃ©es par la tÃ¢che d'ingestion
        # Transformation et chargement
        # Note: Adapter selon la structure rÃ©elle des fonctions
        logger.info("âœ… Transport archive â†’ Neon OK")
    except Exception as e:
        logger.error(f"âŒ ETL transport : {e}")
        raise


# Note: L'entraÃ®nement du modÃ¨le est gÃ©rÃ© par le DAG monitoring_weekly
# qui le dÃ©clenche uniquement si un drift est dÃ©tectÃ© (via TriggerDagRunOperator)


# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•
# DÃ‰FINITION DU DAG
# â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

with DAG(
    dag_id="archive_weekly",
    default_args=default_args,
    description="RÃ©cupÃ©ration hebdomadaire des donnÃ©es historiques (mÃ©tÃ©o + transport)",
    start_date=datetime(2026, 1, 1),
    schedule="0 1 * * 0",  # Dimanche Ã  01:00 (avant monitoring Ã  02:00)
    catchup=False,
    tags=["delay-forecast", "archive", "weekly"],
    max_active_runs=1,  # Une seule exÃ©cution Ã  la fois
) as dag:
    
    start = EmptyOperator(task_id="start")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # INGESTION (en parallÃ¨le)
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    ingest_meteo_archive = PythonOperator(
        task_id="ingest_meteo_archive",
        python_callable=task_ingestion_meteo_archive,
        **api_task_args,
    )
    
    ingest_meteo_forecast = PythonOperator(
        task_id="ingest_meteo_forecast",
        python_callable=task_ingestion_meteo_forecast,
        **api_task_args,
    )
    
    ingest_transport = PythonOperator(
        task_id="ingest_transport_archive",
        python_callable=task_ingestion_transport_archive,
        **api_task_args,
    )
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # ETL
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    etl_meteo = PythonOperator(
        task_id="etl_meteo",
        python_callable=task_etl_meteo,
        **etl_task_args,
    )
    
    etl_transport = PythonOperator(
        task_id="etl_transport",
        python_callable=task_etl_transport,
        **etl_task_args,
    )
    
    end = EmptyOperator(task_id="end", trigger_rule="all_done")
    
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    # DÃ‰PENDANCES
    #
    # start
    #   â”œâ”€â”€ ingest_meteo_archive â”€â”€â”
    #   â”œâ”€â”€ ingest_meteo_forecast â”€â”¼â”€â”€â–º etl_meteo â”€â”€â”€â”€â”€â”
    #   â”‚                          â”‚                   â”œâ”€â”€â–º end
    #   â””â”€â”€ ingest_transport â”€â”€â”€â”€â”€â”€â”´â”€â”€â–º etl_transport â”€â”˜
    #
    # Note: L'entraÃ®nement est dÃ©clenchÃ© par monitoring_weekly si drift
    # â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
    
    start >> [ingest_meteo_archive, ingest_meteo_forecast, ingest_transport]
    
    [ingest_meteo_archive, ingest_meteo_forecast] >> etl_meteo
    ingest_transport >> etl_transport
    
    [etl_meteo, etl_transport] >> end
